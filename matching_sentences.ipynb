{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import string\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import numpy as np\n",
    "import utils\n",
    "path = r'C:\\Users\\faune\\Desktop\\thesis\\stanford_sentiment.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Downloading necessary resources...')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "df = pd.read_parquet(path)\n",
    "print('Initialize the stemmer and stop words')\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stems = [stemmer.stem(word) for word in word_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed'] = df['sentence'].apply(process_text)\n",
    "\n",
    "print('Create a vocabulary of all unique words')\n",
    "vocab = sorted(set(word for tokens in df['processed'].values for word in tokens))\n",
    "\n",
    "print('One-hot encode the words using MultiLabelBinarizer')\n",
    "mlb = MultiLabelBinarizer(classes=vocab)\n",
    "\n",
    "\n",
    "print('Compute quantiles and get the index for the selected quantile')\n",
    "df['len'] = df['processed'].apply(len)\n",
    "q=10\n",
    "df['quantile_len'] = pd.qcut(df['len'],q=q,labels=[f'q{i}' for i in range(1,q+1)])\n",
    "\n",
    "quantile = 'q10'\n",
    "\n",
    "df['positive_idx'] = -1\n",
    "df['negative_idx'] = -1\n",
    "df.loc[(df.label==0) & (df.quantile_len==quantile),'negative_idx'] = df.loc[(df.label==0) & (df.quantile_len==quantile)].reset_index().index.astype('Int64')\n",
    "df.loc[(df.label==1) & (df.quantile_len==quantile),'positive_idx'] = df.loc[(df.label==1) & (df.quantile_len==quantile)].reset_index().index.astype('Int64')\n",
    "\n",
    "print('Compute the gram matrix over the quantile (x positive, y negative)')\n",
    "X = mlb.fit_transform(df.loc[(df['label']==1) & (df.quantile_len==quantile),'processed'])\n",
    "Y = mlb.fit_transform(df.loc[(df['label']==0) & (df.quantile_len==quantile),'processed'])\n",
    "matrix = X@Y.T\n",
    "\n",
    "\n",
    "print('Merging the output to make the correspondance between positive and neg sentences')\n",
    "positive_quantile_df = df.loc[(df.label==1) & (df.quantile_len==quantile),:]\n",
    "negative_quantile_df = df.loc[(df.label==0) & (df.quantile_len==quantile),:]\n",
    "positive_quantile_df['neg_closest_idx'] = np.argmax(matrix,axis=1)\n",
    "merged = positive_quantile_df.merge(negative_quantile_df[['negative_idx','sentence','processed']].rename({'sentence':'neg_sentence','processed':'neg_processed'},axis=1),\n",
    "                            how='left',\n",
    "                            left_on='neg_closest_idx',\n",
    "                            right_on='negative_idx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pl.read_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sentences = (\n",
    "    pl.DataFrame(merged)\n",
    "    .select('sentence', 'processed', 'neg_sentence', 'neg_processed')\n",
    "    .join(embeddings, on = 'sentence')\n",
    "    .drop('idx')\n",
    "    .join(embeddings.rename({'sentence': 'neg_sentence'}), on = 'neg_sentence', suffix = '_neg')\n",
    "    .select(pl.exclude('idx', 'label', 'label_neg'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.display_polars(\n",
    "    matched_sentences\n",
    "    .unique()\n",
    "    .with_columns(pl.col('processed').list.set_intersection(pl.col('neg_processed')).alias('matching_words'))\n",
    "    .with_columns(pl.col('matching_words').list.len().alias('nb'))\n",
    "    .with_columns(\n",
    "            pl.col('nb').truediv(pl.col('processed').list.len()).alias('ratio_pos'), \n",
    "            pl.col('nb').truediv(pl.col('neg_processed').list.len()).alias('ratio_neg')\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.mean_horizontal('ratio_pos', 'ratio_neg').alias('ratio')\n",
    "    )\n",
    "    .sort('ratio', descending=True)\n",
    "    .filter(pl.col('ratio').gt(0.2), pl.col('ratio').lt(0.8))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_metrics = (matched_sentences.unique().with_columns(\n",
    "        matched_sentences.with_columns(pl.col('embeddings').list.to_array(1024),pl.col('embeddings_neg').list.to_array(1024))\n",
    "        .map_rows(function=lambda t : tuple(cosine_similarity(X=np.array(t[4]).reshape(1, -1), Y=np.array(t[5]).reshape(1, -1)).tolist()))\n",
    "    )\n",
    "    .explode('column_0')\n",
    "    .sort('column_0')\n",
    "    .rename({'column_0': 'cosine_similarity'})\n",
    "    .with_columns(\n",
    "        matched_sentences.with_columns(pl.col('embeddings').list.to_array(1024),pl.col('embeddings_neg').list.to_array(1024))\n",
    "        .map_rows(function=lambda t :np.dot(a=t[4], b=np.transpose(t[5])))\n",
    "    )\n",
    "    .rename({'map': 'dot_product'})\n",
    "    .sort('cosine_similarity')\n",
    "    .with_columns(\n",
    "        matched_sentences.with_columns(pl.col('embeddings').list.to_array(1024),pl.col('embeddings_neg').list.to_array(1024))\n",
    "        .map_rows(function=lambda t : tuple(euclidean_distances(X=np.array(t[4]).reshape(1, -1), Y=np.array(t[5]).reshape(1, -1)).tolist()))\n",
    "    )\n",
    "    .explode('column_0')\n",
    "    .rename({'column_0':'euclidean_distance'})\n",
    "    .with_columns(\n",
    "        matched_sentences.with_columns(pl.col('embeddings').list.to_array(1024),pl.col('embeddings_neg').list.to_array(1024))\n",
    "        .map_rows(function=lambda t :np.linalg.norm(x=t[4]) -np.linalg.norm(x=t[5]))\n",
    "    )\n",
    "    .rename({'map':'norm_difference'})\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "\n",
    "sns.histplot(data=with_metrics.to_pandas(), x='euclidean_distance', kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=with_metrics.to_pandas(), x='cosine_similarity', kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=with_metrics.to_pandas(), x='dot_product', kde = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=with_metrics.to_pandas(), x='norm_difference', kde = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mistralai\n",
    "import mistralai.async_client\n",
    "import mistralai.client\n",
    "from api_key import API_KEY\n",
    "\n",
    "client = mistralai.client.MistralClient(api_key=API_KEY)\n",
    "test = client.embeddings(model=\"mistral-embed\", input=\"hello there, how are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([d.embedding for d in test.data][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_latex\n",
    "display_latex(with_metrics.describe().to_pandas(), raw = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(with_metrics.select('cosine_similarity', 'euclidean_distance', 'norm_difference').describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
