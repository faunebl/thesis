---
title: "Do LLMs encode financial sentiment ? An analysis of financial headlines embeddings"
format:
  pdf:
    code-annotations: true
    documentclass: article
    colorlinks: true
    latex-auto-install: true
    classoption: a4paper
    geometry: margin=1in
    citation-style: apa
    keep-tex: true
    toc: true
    toc-depth: 4
    number-sections: true
css: |
  p {
    text-align: justify;
  }
jupyter:
  kernelspec: 
    name: python3
    language: python
    display_name: python3
---

## Introduction

## A broad overview of transformer models

Like any model, transformers are defined by three main characteristics: the type of the input data, the set of transformations to that data, and the output of the model. The first important thing to realize is that the input data to a transformer will always be a sequence of "tokens". A token is actually a numerical vector with a fixed dimension, which we will denote $D$. Therefore, the total input data to a transformer is a matrix with dimensions $D \times N$, where $N$ is the number of tokens. This means that textual data has to be transformed into a matrix of tokens in order to be fed into the transformer. This is called *subword tokenization.* Although we won't delve into the specifics, the main thing to understand is that common word segments such as "ing", "ood" or "un" (as in believ*ing*, believ*ed*, *un*believable) , as well as common words ( "as", "the", "a"), are mapped to a set of vectors called the embedding matrix. This matrix can either be fixed, or learned in order to optimize the model. Therefore, when receiving a sentence, the model will split it into $N$ parts which will all have a corresponding vector in the embedding matrix. Once this is done, a set of transformations will be applied to the $D \times N$ matrix, and the output, in the form of a single $D$-dimensional vector, will be generated. This final vector is what we call an *embedding*.

In this part, we will give the reader a broad understanding of the transformer architecture. First of all, we will give some formal definitions for the mathematical transformations to the data, in order to summarize the general mechanism of a transformer model. Then, we will present an intuitive explanation to sentence embeddings. Finally, we will go over the main mathematical methods used to analyze embeddings.

### [The transformer block: formal mathematical description]{.underline}

The transformer block is the name given to the set of operations which is iteratively applied to the input sequence in order to produce the output of the model. It is comprised of two main stages :

1.  **Multi-Head Self-Attention**

2.  **Multi-Layer Perceptron**

They are connected together with *residual connections* and *token normalization*.

For the sake of simplicity, we will only go into details about the mechanisms of stage one, as it is the true innovation of transformer models and the main way to understand how embeddings encode meaning.

#### *Multi-Head Self-Attention*

First of all, let's dig into what "attention" is, in the simplest possible terms. We can visualize the $D \times N$ matrix as follows:

```{python}
#| echo: false
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle

fig, ax = plt.subplots()
shape = Rectangle((5,5),
                 width=3,
                 height=5,
                 fill = False)
highlight = Rectangle((7,5), width= 0.1, height = 5, fill = True, color = 'red', ec = 'black', label = "Token n")
feature = Rectangle((7,7), width= 0.1, height = 0.2, fill = True, color = 'green', ec = 'black', label = "Feature d")
ax.add_patch(shape)
ax.add_patch(highlight)
ax.add_patch(feature)
ax.autoscale_view()
ax.relim()
plt.axis('off')
plt.arrow(x=4.8, y=5, dx=0, dy=5, length_includes_head = True,head_width = 0.03, color='black')
plt.arrow(x=5, y=10.4, dx=3, dy=0, color='black',width = 0.002, head_width = 0.06, length_includes_head = True, head_length = 0.03)
ax.annotate("", xy=(0.5, 0.5), xytext=(0, 0),
            arrowprops=dict(arrowstyle="->"))
plt.text(x=4.6, y = 7, s = 'D')
plt.text(x=7, y = 10.6, s = 'N')
plt.legend(loc=(1, 0.5))
plt.show()
```

Where each token has $D$ features.

Let $x_n$ denote the input vector at the $n^{\text{th}}$ location, and $y_n$ denote the output vector at the $n^{\text{th}}$ location. The so-called "attention" operation performs a weighted average such that:

$$
y_n = \sum_{n' = 1}^{N} x_{n'} \times A_{n',n}
$$

Where $A_{n',n}$ is the attention matrix or size $N \times N$. This means that each feature for each token will be refined by the value of this same feature among all other tokens in the input matrix. The attention matrix gives the weights for each vector. For example, in the sentence "The cat eats the blue flower", the weights would be spread out so that the model can understand that the word "blue" relates to the word "flower", and so that the verb "eats" relates to the subject "cat". If we can imagine that feature $d$ represents the "color" dimension for a noun, the attention matrix would be constructed so that the relationship between the words "blue" ($x_{n'}$) and "cat" ($x_n$) would be very low. Therefore, a low value for $A_{n',n}$ would allow the output vector $y_n$ to reflect that the sentence does not give any information about the color of the cat.

Now, we are going to present how the attention matrix is actually created. We use the expression "self" attention because the attention matrix is computed from the values in the input matrix. This works because the importance of each token in the contextual meaning of another token depends on the structure of the sentence. For example, we know that adjectives apply to nouns, and in a sentence with multiple nouns, we can say that one particular adjective relates to the nearest noun. We therefore need to imagine that information about the location of the words in the sentence, as well as the grammatical types of the words, are all encoded into the initial token embeddings. Therefore, we can compute the attention matrix by computing the similarity between locations in the sequence of tokens. However, attention needs to capture which words are important to which, but it is not used to refine the meaning (i.e. the content) of the tokens themselves. We therefore need to compute attention without integrating information about the content of the vectors. Moreover, the relationship between tokens should be asymmetric.

Let us define the general formula for the attention matrix as:

$$
A_{n,n'} = \frac{
exp(
\frac{x_n^T U_k^T U_q x_{n'}}{\sqrt{D}}
)
}
{
\sum_{n''=1}^N 
exp(
\frac{x_{n''}^T U_k^T U_q x_{n'}}{\sqrt{D}}
)
}
$$

Where $U_q$ and $U_k$ are $K \times D$ dimensional matrices, where $K < D$. These matrices asymmetrically perform linear transforms on the sequence so that some features are not included in the computation of the similarity between vectors.

In order to understand this equation, we will focus on explaining each part separately.

First of all, all values in the attention matrix should be between 0 and 1, since it is used to compute a weighted average. We call *softmax* the function that takes a vector of real numbers as an input, and normalizes it to a probability distribution. This means that each number in the vector is between 0 and 1, and that they all sum up to exactly 1. This function is formally defined as:

```{=tex}
\begin{align*}
& \sigma : \mathbb{R}^D \rightarrow (0,1)^D \\
& \sigma(x)_n = \frac{exp(x_n)}{\sum_{n'' = 1}^N exp(x_{n''})}
\end{align*}
```
We can therefore recognize that the attention matrix is effectively the result of applying a softmax function to a linear transform of the input vectors. This linear transform can be written:

```{=tex}
\begin{align*}
& f : \mathbb{R}^D \times \mathbb{R}^D \rightarrow \mathbb{R}^D \\
& f(x_n, x_{n'}) = \frac{x_n^T \cdot U_k^T \cdot U_q \cdot x_{n'}}{\sqrt{D}}
\end{align*}
```
We divide the result of the projection to a lower dimensional space by $\sqrt{D}$ for numerical stability. The matrices $U_k$ and $U_q$ are parameters of the model, and they will be computed during training.

Now that we have defined what "self-attention" means, the final piece of the puzzle to understand the first stage of the transformer block is to show how the limits of this mechanism have been solved. In this architecture, we can see that the similarity between the vectors is computed across all vector features. This means that for each feature, we use the same value to determine how much "attention" we should pay to all other vectors in the sequence. This causes a problem. For example, what if we want the $d^{\text{th}}$ feature of token $x_n$ to be important to the $d^{\text{th}}$ feature of token $x_{n'}$, but we don't want the $d'^{\text{ th}}$ feature of token $x_n$ to be important to the $d'^{\text{ th}}$ feature of token $x_{n'}$ ?

For this reason, the transformer block computes a number of self-attention in parallel, by splitting the original input matrix into $H$ bits, where $H$ is the number of self-attention mechanisms. One self-attention computation is called a *head*. This therefore brings us to the term *multi-head self-attention*.

The formal mathematical definition of multi-head self-attention, in matrix representation is therefore:

$$
Y = \sum_{h=1}^H V_h \cdot X \cdot A_h
$$

Where $V_h$ is a $D \times D$ matrix used to project the sliced $X$ (into $H$ bits) back to the original dimensionality $D$. Each attention matrix $A_h$ is also computed on each slice of $X$.

Of course, this is a simplified presentation of attention, as there are many details that we are not able to dig into here. Let's now move on to the second stage of the transformer block.

#### *Multi-Layer Perceptron*

Although we won't go into the specifics of what a Multi-Layer Perceptron (MLP) is mathematically, we will give a short bit of intuition into why it is used in the transformer block.

The first thing to notice is that the attention mechanism is applied *horizontally* across the different vectors, while the Multi-Layer Perceptron is applied *vertically* across features. The MLP is a feed-forward neural network which is composed of linear layers separated by non-linear activation functions, which allow the model to capture complex non-linear relationships in the data. It can therefore encode the relationship between the features in the original vector. This allows it to learn more abstract representations of the input data, and specify the information that it has learned in the attention stage.

#### *Residual connections and token normalization*

They are the elements that allow the transformer block to connect the two stages (MHSA and MLP) together.

**Residual connections**

:   They are functions which model the residual differences between each time we iteratively apply the transformer block. They are a non-linear function that is close to the identity function. We can write them formally as follows : $x^m - x^{m-1} = res(x^{m-1})$ where $m$ is the iteration of each block.

Token Normalization

:   The function simply normalizes each token by removing its mean and dividing by its standard deviation. Although, they also present additional parameters in order to scale and shift the data. Its formal mathematical definition is :

    $$
    \bar{x}_{d,n} = \frac{(x_{d,n} - \bar{x}_n) a_d}{\sqrt{var(x_n)}} + b_d
    $$

    Where $a_d$ is the scale parameter and $b_d$ is the shift parameter.

    This is applied to each feature for each token

We can therefore present the general architecture for the transformer :

```{mermaid}
%%| fig-height: 3.5
%%| fig-width: 6.5
graph TB
I[(Input)] --> E(Embeddings)
S2(Stage 2 Output) -- repeat --> E
E -- token_norm --> MHSA(MHSA)
MHSA{{MHSA}} -- res_connect --> S1(Stage 1 Output)
S1 -- token_norm --> MLP{{MLP}}
MLP -- res_connect --> S2
S2 --> F(Final Output)

classDef blue fill:#663196,stroke:#20152a,color:#ffffff;
class I,F blue;
classDef green fill:#a58eb9,stroke:#20152a,color:#ffffff;
class E,S1,S2 green;
classDef orange fill:#e88d67,stroke:#20152a,color:#ffffff;
class MHSA,MLP orange;
```

### [An intuitive understanding of embeddings]{.underline}

Now that we have some mathematical background on how transformers generate and edit embeddings, we will give some quick intuition into what this means in practice, with applied examples.

Let's therefore use a popular example which is often used to show the practical use of embeddings. It consists in calculating the euclidian distance between the embeddings for the words "Man" and "Woman", and then comparing it to the euclidian distance between the masculine and feminine version of the same word. There are a number of variations for this example. Here, we will compare the distance between the names of two famous economists, and the distance between their most well-known books

```{python}
#| echo: true
from api_key import API_KEY
from mistralai.client import MistralClient
from sklearn.metrics.pairwise import euclidean_distances

client = MistralClient(api_key=API_KEY)
def get_euclidian_distance(x: str, y: str) -> float:
  embed_x = client.embeddings(input=x, model="mistral-embed").data[0].embedding
  embed_y = client.embeddings(input=y, model="mistral-embed").data[0].embedding
  return euclidean_distances(X=[embed_x], Y=[embed_y])[0][0]

print(get_euclidian_distance('John Maynard Keynes', 'Adam Smith'))
print(get_euclidian_distance('The General Theory of Employment, Interest and Money', 'The Wealth Of Nations'))
```

Here, we can clearly see that the distance between the economists and their books is comparable. Although this could be a simple coincidence, it serves as an illustration as to what embeddings are.

We can illustrate this further by reducing the dimension of our embeddings vectors and plotting them. This can help us to visualize the distance between embeddings.

```{python}
#| echo: false
import seaborn as sns
from sklearn.manifold import TSNE
import numpy as np
import polars as pl 

def get_labeled_embedding(text_list: list) -> pl.DataFrame:
  labelled = []
  for word in text_list:
      embed = client.embeddings(input=word, model="mistral-embed").data[0].embedding
      labelled.append(
        {
          'text':word,
          'embeddings': embed
        }
      )
  return pl.DataFrame(labelled)

df = get_labeled_embedding(['John Maynard Keynes', 'Adam Smith', 'The General Theory of Employment, Interest and Money', 'The Wealth Of Nations'])

tsne = TSNE(n_components=2, random_state=0, perplexity=3).fit_transform(np.array(df['embeddings'].to_list()))
ax = sns.scatterplot(x=tsne[:, 0], y=tsne[:, 1], hue=np.array(df['text'].to_list()))
sns.move_legend(ax, 'upper left', bbox_to_anchor=(1, 1))

```

### [Mathematical methods to analyze embeddings]{.underline}

## Analyzing embeddings

### [K-means clustering on labeled data]{.underline}

### [Cosine similarity and euclidean distances on dated data]{.underline}

### [Comparing results]{.underline}

## Conclusion

## Bibliography

## Annexe