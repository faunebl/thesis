---
title: "Do LLMs encode financial sentiment ? An analysis of financial headlines embeddings"
format:
  pdf:
    code-annotations: true
    documentclass: article
    colorlinks: true
    latex-auto-install: true
    classoption: a4paper
    geometry: margin=1in
    citation-style: apa
    keep-tex: true
    toc: true
    toc-depth: 4
    number-sections: true
css: |
  p {
    text-align: justify;
  }
jupyter:
  kernelspec: 
    name: python3
    language: python
    display_name: python3
---

\newpage

```{=tex}
\newcommand{\icol}[1]{% inline column vector
  \left(\begin{smallmatrix}#1\end{smallmatrix}\right)%
}

\newcommand{\irow}[1]{% inline row vector
  \begin{smallmatrix}(#1)\end{smallmatrix}%
}
```
## Introduction

When you ask a finance professional what makes a good trader, they often talk about the ability to "feel the market". To the young aspiring trader, this might not seem to make much sense, or to have a clear meaning. What traders probably mean when they say this, is that some people have an intuitive understanding of market phenomenons, or at least an innate ability to recognize and memorize complex patterns in market participants' behavior. This, in itself, gives them an advantage in predicting future events on markets. Of course, there are many more things that go into being a great trader : rigor, risk appetite, raw financial knowledge, etc. But "feeling the market" is perhaps the most vague, and therefore the most interesting.

In what might be an effort to replicate this intuition, research fpr the past few years has tried to analyze sentiment in financial text. The goal would be to have a certain short text as an input, and extract a trading signal from it. This has been executed in more or less sophisticated ways, with a varying degree of success. However, this practice has been revolutionized by the appearance of Large Language Models (LLMs), which are phenomenal at understanding meaning and syntax in text. These machine learning models are used to create classifiers, which can give an estimate of the sentence's sentiment. The results go from -1 to 1, where -1 represents negative sentiment and 1 positive sentiment. Different LLMs have different levels of accuracy, but sentence sentiment predicted in this way generally replicates sentiment labels attributed to sentences by humans. Therefore, previous research has tried to use the resulting sentiment index in order to compare the results with movements in markets.

However, we hypothesize that there might be another way to use these models in order to get information about markets from financial headlines. As previously mentioned, in order to extract sentiment from sentences, LLMs transform a sentence into an n-dimensional vector, and then use classifiers in order to create a probability distribution for the sentiment indicator, using the sentence vector. But, we think that there might be other relevant information in this vector that is not read by the model. For instance, not all news sentiment is similarly relevant to markets. If we use sentiment from tweets to predict markets for example, how can we understand that a single Elon Musk tweet can have more effect on the price of a certain cryptocurrency than some FED meetings have on S&P500 prices ? This example is a little vulgarized, but in short, we will try to see if raw contextual information can be used to trade stocks. Perhaps, the model might encode something about sentences that could be invisible to humans but be relevant to markets ?

In order to do this, we have developed a short methodology. First, we need to understand how sentiment is encoded by the model. This means that we will visualize the sentence vectors, and try to extract a relationship between these vectors and sentiment. This is a sort of check to see if the model actually puts sentences with different sentiments in different subspaces, similar to the research that has been done on word meaning and sentence syntax. Then, we will need to see if a similar phenomenon occurs when sentiment labels are given by raw market information, such as a certain return for an index. The first analysis allows us to pinpoint what sort of efficient relationship we are looking to see in the second analysis, since we know that the model is successful at representing sentiment, and that what we are trying to see is theoretically close to what sentiment is.

Therefore, this thesis will be structured in the following way. Firstly, we will give the reader some understanding of how the model creates meaningful vectors our of sentences. This will comprise in a description of the mathematical functions used in the model, and in an intuitive illustration of the results. Secondly, we will proceed with our analysis, which is comprised of the two parts previously presented. Then, we will try to reach a conclusion to our initial hypothesis.

## A broad overview of transformer models

Like any model, transformers are defined by three main characteristics: the type of the input data, the set of transformations to that data, and the output of the model. The first important thing to realize is that the input data to a transformer will always be a sequence of "tokens". A token is actually a numerical vector with a fixed dimension, which we will denote $D$. Therefore, the total input data to a transformer is a matrix with dimensions $D \times N$, where $N$ is the number of tokens. This means that textual data has to be transformed into a matrix of tokens in order to be fed into the transformer. This is called *subword tokenization.* Although we won't delve into the specifics, the main thing to understand is that common word segments such as "ing", "ood" or "un" (as in believ*ing*, believ*ed*, *un*believable) , as well as common words ( "as", "the", "a"), are mapped to a set of vectors called the embedding matrix. This matrix can either be fixed, or learned in order to optimize the model. Therefore, when receiving a sentence, the model will split it into $N$ parts which will all have a corresponding vector in the embedding matrix. Once this is done, a set of transformations will be applied to the $D \times N$ matrix, and the output, in the form of a single $D$-dimensional vector, will be generated. This final vector is what we call an *embedding*.

In this part, we will give the reader a broad understanding of the transformer architecture. First of all, we will give some formal definitions for the mathematical transformations to the data, in order to summarize the general mechanism of a transformer model. Then, we will present an intuitive explanation to sentence embeddings. Finally, we will go over the main mathematical methods used to analyze embeddings.

### [The transformer block: formal mathematical description]{.underline}

The transformer block is the name given to the set of operations which is iteratively applied to the input sequence in order to produce the output of the model. It is comprised of two main stages :

1.  **Multi-Head Self-Attention**

2.  **Multi-Layer Perceptron**

They are connected together with *residual connections* and *token normalization*.

For the sake of simplicity, we will only go into details about the mechanisms of stage one, as it is the true innovation of transformer models and the main way to understand how embeddings encode meaning. For the sake of simplicity, we will only go into details about the mechanisms of stage one, as it is the true innovation of transformer models and the main way to understand how embeddings encode meaning.

#### *Multi-Head Self-Attention*

First of all, let's dig into what "attention" is, in the simplest possible terms. We can visualize the $D \times N$ matrix as follows:

```{python}
#| echo: false
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle

fig, ax = plt.subplots()
shape = Rectangle((5,5),
                 width=3,
                 height=5,
                 fill = False)
highlight = Rectangle((7,5), width= 0.1, height = 5, fill = True, color = 'red', ec = 'black', label = "Token n")
feature = Rectangle((7,7), width= 0.1, height = 0.2, fill = True, color = 'green', ec = 'black', label = "Feature d")
ax.add_patch(shape)
ax.add_patch(highlight)
ax.add_patch(feature)
ax.autoscale_view()
ax.relim()
plt.axis('off')
plt.arrow(x=4.8, y=5, dx=0, dy=5, length_includes_head = True,head_width = 0.03, color='black')
plt.arrow(x=5, y=10.4, dx=3, dy=0, color='black',width = 0.002, head_width = 0.06, length_includes_head = True, head_length = 0.03)
ax.annotate("", xy=(0.5, 0.5), xytext=(0, 0),
            arrowprops=dict(arrowstyle="->"))
plt.text(x=4.6, y = 7, s = 'D')
plt.text(x=7, y = 10.6, s = 'N')
plt.legend(loc=(1, 0.5))
plt.show()
```

Where each token has $D$ features.

Let $x_n$ denote the input vector at the $n^{\text{th}}$ location, and $y_n$ denote the output vector at the $n^{\text{th}}$ location. The so-called "attention" operation performs a weighted average such that:

$$
y_n = \sum_{n' = 1}^{N} x_{n'} \times A_{n',n}
$$

Where $A_{n',n}$ is the attention matrix or size $N \times N$. This means that each feature for each token will be refined by the value of this same feature among all other tokens in the input matrix. The attention matrix gives the weights for each vector. For example, in the sentence "The cat eats the blue flower", the weights would be spread out so that the model can understand that the word "blue" relates to the word "flower", and so that the verb "eats" relates to the subject "cat". If we can imagine that feature $d$ represents the "color" dimension for a noun, the attention matrix would be constructed so that the relationship between the words "blue" ($x_{n'}$) and "cat" ($x_n$) would be very low. Therefore, a low value for $A_{n',n}$ would allow the output vector $y_n$ to reflect that the sentence does not give any information about the color of the cat.

Now, we are going to present how the attention matrix is actually created. We use the expression "self" attention because the attention matrix is computed from the values in the input matrix. This works because the importance of each token in the contextual meaning of another token depends on the structure of the sentence. For example, we know that adjectives apply to nouns, and in a sentence with multiple nouns, we can say that one particular adjective relates to the nearest noun. We therefore need to imagine that information about the location of the words in the sentence, as well as the grammatical types of the words, are all encoded into the initial token embeddings. Therefore, we can compute the attention matrix by computing the similarity between locations in the sequence of tokens. However, attention needs to capture which words are important to which, but it is not used to refine the meaning (i.e. the content) of the tokens themselves. We therefore need to compute attention without integrating information about the content of the vectors. Moreover, the relationship between tokens should be asymmetric.

Let us define the general formula for the attention matrix as:

$$
A_{n,n'} = \frac{
exp(
\frac{x_n^T U_k^T U_q x_{n'}}{\sqrt{D}}
)
}
{
\sum_{n''=1}^N 
exp(
\frac{x_{n''}^T U_k^T U_q x_{n'}}{\sqrt{D}}
)
}
$$

Where $U_q$ and $U_k$ are $K \times D$ dimensional matrices, where $K < D$. These matrices asymmetrically perform linear transforms on the sequence so that some features are not included in the computation of the similarity between vectors.

In order to understand this equation, we will focus on explaining each part separately.

First of all, all values in the attention matrix should be between 0 and 1, since it is used to compute a weighted average. We call *softmax* the function that takes a vector of real numbers as an input, and normalizes it to a probability distribution. This means that each number in the vector is between 0 and 1, and that they all sum up to exactly 1. This function is formally defined as:

```{=tex}
\begin{align*}
& \sigma : \mathbb{R}^D \rightarrow (0,1)^D \\
& \sigma(x)_n = \frac{exp(x_n)}{\sum_{n'' = 1}^N exp(x_{n''})}
\end{align*}
```
We can therefore recognize that the attention matrix is effectively the result of applying a softmax function to a linear transform of the input vectors. This linear transform can be written:

```{=tex}
\begin{align*}
& f : \mathbb{R}^D \times \mathbb{R}^D \rightarrow \mathbb{R}^D \\
& f(x_n, x_{n'}) = \frac{x_n^T \cdot U_k^T \cdot U_q \cdot x_{n'}}{\sqrt{D}}
\end{align*}
```
We divide the result of the projection to a lower dimensional space by $\sqrt{D}$ for numerical stability. The matrices $U_k$ and $U_q$ are parameters of the model, and they will be computed during training.

Now that we have defined what "self-attention" means, the final piece of the puzzle to understand the first stage of the transformer block is to show how the limits of this mechanism have been solved. In this architecture, we can see that the similarity between the vectors is computed across all vector features. This means that for each feature, we use the same value to determine how much "attention" we should pay to all other vectors in the sequence. This causes a problem. For example, what if we want the $d^{\text{th}}$ feature of token $x_n$ to be important to the $d^{\text{th}}$ feature of token $x_{n'}$, but we don't want the $d'^{\text{ th}}$ feature of token $x_n$ to be important to the $d'^{\text{ th}}$ feature of token $x_{n'}$ ?

For this reason, the transformer block computes a number of self-attention in parallel, by splitting the original input matrix into $H$ bits, where $H$ is the number of self-attention mechanisms. One self-attention computation is called a *head*. This therefore brings us to the term *multi-head self-attention*.

The formal mathematical definition of multi-head self-attention, in matrix representation is therefore:

$$
Y = \sum_{h=1}^H V_h \cdot X \cdot A_h
$$

Where $V_h$ is a $D \times D$ matrix used to project the sliced $X$ (into $H$ bits) back to the original dimensionality $D$. Each attention matrix $A_h$ is also computed on each slice of $X$.

Of course, this is a simplified presentation of attention, as there are many details that we are not able to dig into here. Let's now move on to the second stage of the transformer block.

#### *Multi-Layer Perceptron*

Although we won't go into the specifics of what a Multi-Layer Perceptron (MLP) is mathematically, we will give a short bit of intuition into why it is used in the transformer block.

The first thing to notice is that the attention mechanism is applied *horizontally* across the different vectors, while the Multi-Layer Perceptron is applied *vertically* across features. The MLP is a feed-forward neural network which is composed of linear layers separated by non-linear activation functions, which allow the model to capture complex non-linear relationships in the data. It can therefore encode the relationship between the features in the original vector. This allows it to learn more abstract representations of the input data, and specify the information that it has learned in the attention stage.

#### *Residual connections and token normalization*

They are the elements that allow the transformer block to connect the two stages (MHSA and MLP) together.

**Residual connections**

:   They are functions which model the residual differences between each time we iteratively apply the transformer block. They are a non-linear function that is close to the identity function. We can write them formally as follows : $x^m - x^{m-1} = res(x^{m-1})$ where $m$ is the iteration of each block.

Token Normalization

:   The function simply normalizes each token by removing its mean and dividing by its standard deviation. Although, they also present additional parameters in order to scale and shift the data. Its formal mathematical definition is :

    $$
    \bar{x}_{d,n} = \frac{(x_{d,n} - \bar{x}_n) a_d}{\sqrt{var(x_n)}} + b_d
    $$

    Where $a_d$ is the scale parameter and $b_d$ is the shift parameter.

    This is applied to each feature for each token

We can therefore present the general architecture for the transformer :

```{mermaid}
%%| fig-height: 3.5
%%| fig-width: 6.5
graph TB
I[(Input)] --> E(Embeddings)
S2(Stage 2 Output) -- repeat --> E
E -- token_norm --> MHSA(MHSA)
MHSA{{MHSA}} -- res_connect --> S1(Stage 1 Output)
S1 -- token_norm --> MLP{{MLP}}
MLP -- res_connect --> S2
S2 --> F(Final Output)

classDef blue fill:#663196,stroke:#20152a,color:#ffffff;
class I,F blue;
classDef green fill:#a58eb9,stroke:#20152a,color:#ffffff;
class E,S1,S2 green;
classDef orange fill:#e88d67,stroke:#20152a,color:#ffffff;
class MHSA,MLP orange;
```

Of course, this is a simplified presentation of attention, as there are many details that we are not able to dig into here. Let's now move on to the second stage of the transformer block.

### [An intuitive understanding of embeddings]{.underline}

Now that we have some mathematical background on how transformers generate and edit embeddings, we will give some quick intuition into what this means in practice, with applied examples. We have shown that the model is able to mathematically extract meaningful and useful features from its input data. Therefore, we will now show how that translates to the model's internal representation of those features.

Let's therefore use a popular example which is often used to show the practical use of embeddings. It consists in calculating the euclidian distance between the embeddings for the words "Man" and "Woman", and then comparing it to the euclidian distance between the masculine and feminine version of the same word. There are a number of variations for this example. Here, we will compare the distance between the names of two famous economists, and the distance between their most well-known books

```{python}
#| echo: true
from api_key import API_KEY
from mistralai.client import MistralClient
from sklearn.metrics.pairwise import euclidean_distances

client = MistralClient(api_key=API_KEY)
def get_euclidian_distance(x: str, y: str) -> float:
  embed_x = client.embeddings(input=x, model="mistral-embed").data[0].embedding
  embed_y = client.embeddings(input=y, model="mistral-embed").data[0].embedding
  return euclidean_distances(X=[embed_x], Y=[embed_y])[0][0]

print(
  get_euclidian_distance('John Maynard Keynes', 'Adam Smith')
)
print(
  get_euclidian_distance(
    'The General Theory of Employment, Interest and Money', 
    'The Wealth Of Nations'
  )
)
```

Here, we can clearly see that the distance between the economists and their books is comparable. Although this could be a simple coincidence, it serves as an illustration as to what embeddings are.

We can illustrate this further by reducing the dimension of our embeddings vectors and plotting them. This can help us to visualize what the euclidian distance means in terms of the model's understanding of word's meanings. It can also help us to see that this might not be just a simple coincidence.

```{python}
#| echo: true
import seaborn as sns
from sklearn.manifold import TSNE
import numpy as np
import polars as pl 

def get_labeled_embedding(text_list: list) -> pl.DataFrame:
  labelled = []
  for word in text_list:
      embed = client.embeddings(input=word, model="mistral-embed").data[0].embedding
      labelled.append(
        {
          'text':word,
          'embeddings': embed
        }
      )
  return pl.DataFrame(labelled)

df = get_labeled_embedding(
  [
    'John Maynard Keynes', 
    'Adam Smith', 
    'The General Theory of Employment, Interest and Money', 
    'The Wealth Of Nations'
  ]
)

tsne = TSNE(n_components=2, random_state=0, perplexity=3)
tsne = tsne.fit_transform(np.array(df['embeddings'].to_list()))
ax = sns.scatterplot(x=tsne[:, 0], y=tsne[:, 1], hue=np.array(df['text'].to_list()))
sns.move_legend(ax, 'upper left', bbox_to_anchor=(1, 1))

```

It is important to note that before being able to visualize embeddings in such a way, we have to reduce the dimension of each vector. Here, we have done this using T-SNE as a dimension reduction technique. This can also be done with a standard PCA. We will present the different mathematical methods that allow us to simplify out analysis of embeddings, such as T-SNE in section $1.3$.

Here, we can clearly see that the four points form an almost perfect square, meaning that the model seems to encode the difference between the two economists' theories. However, it also seems that there could be a linear transform that allows us to go from an economist to its most famous work.

This visualization also helps us to see why cosine similarity is important in measuring embeddings' meaning. Even though singular embeddings here are represented as points and not as vectors, we can use the graph to show the intuition behind the importance of cosine similarity. Indeed, it's not just that the euclidian distance between economists and the distance between their books is similar, it's also that the vectors formed by linking economists and their books *point in the same direction*.

This can easily be checked :

```{python}
from sklearn.metrics.pairwise import cosine_similarity
display(df.drop('embeddings').with_columns(pl.Series(name = 'tsne', values = tsne)))
print(cosine_similarity(X= tsne[1:2,:] - tsne[3:4,:], Y= tsne[0:1,:] - tsne[2:3,:] ))
print(cosine_similarity(X= tsne[0:1,:] - tsne[1:2,:], Y= tsne[2:3,:] - tsne[3:4,:] ))

```

We can therefore see that the vectors linking economists to their books have a cosine similarity of 1, and the vectors between economists and between their books have a cosine similarity of nearly 1.

The last way to measure similarity between vectors is to use their dot product. This is sort of a hybrid between the cosine similarity (which doesn't take into account the vectors' size), and the euclidian distance between points (which measures the size of the vector between two points). We can compute the dot product of the vectors as follows :

```{python}
print(np.dot(a=tsne[1:2,:] - tsne[3:4,:], b= np.transpose(tsne[0:1,:] - tsne[2:3,:])))
print(np.dot(a= tsne[0:1,:] - tsne[1:2,:], b= np.transpose(tsne[2:3,:] - tsne[3:4,:])))
```

This is also the similarity measures that we have seen used during the self-attention process of the transformer. In order to analyze the meaning of the dot product, we can look at the mathematical formula :

$$
u \cdot v = \lvert u \rvert \times \lvert v \rvert \times cos(\theta)
$$

Where $\lvert u \rvert$ is the euclidean norm of $u$ and $cos(\theta)$ is the cosine similarity between $u$ and $v$. We can therefore see that the cosine similarity penalizes the length of the two vectors. Therefore, the higher the value, the more similar the vectors are.

Keep in mind that this example is purely anecdotal to give the reader some basic intuition into how one should understand token embeddings. Of course, a rigorous analysis must imply a wide range of data points. The dimension reduction technique and the similarity metrics between vectors introduce major biases in the results. However, it is still useful to get a broad understanding of how embeddings are supposed to represent meaning.

### [Mathematical methods to analyze embeddings]{.underline}

We are now going to present the main methods that are used in the literature to measure how the model encodes the meaning of words. Since we have seen that word sense is represented by location in space, we should therefore be able to find where each sentence is in space, and therefore test if that corresponds to the meaning of the sentence. This is done in multiple papers, however, they mostly focus on syntax instead of meaning. Our analysis throughout this thesis will be even more different, as we will focus on *sentiment* in sentences, and later on the link between embeddings and markets. Here we will quickly present two techniques which are necessary in order to be able to perform any analysis on embeddings. We will use both of them throughout this thesis.

#### *Dimension reduction*

Dimension reduction algorithms allow us to simply reduce the number of dimension that a certain vector has, in order to have a more readable outcome. Indeed, sentence embeddings vectors generally have between 700 and 1200 rows. It can therefore be very hard to understand what that means for us humans, since we can only really think in three dimensions. We can think of dimension reduction just as drawing a 3D cube on paper. It's a method to transfer information from other dimensions in a lower dimensional space. However, algorithms are not all equal in their ability to preserve the global structure of the initial data. The most famous dimension reduction methods are PCA (Principal Component Analysis) and t-SNE (T-distributed Stochastic Neighbor Embedding). Generally, there are two types of dimension reduction techniques. There are those that want to preserve the distance among all of the different pairs of samples (PCA), and those that try to keep the local distances identical over the global distances (t-SNE). We will present both of those algorithms here, although we will only give the intuition behind t-SNE as the mathematics behind it are a little more complex. It is however important to understand both of those methods in order to understand our analysis.

t-SNE

:   t-SNE is based on an earlier technique called Stochastic Neighbor Embedding, but with a few tweaks to make it more effective. It is non-linear. It works in two main stages. First, it calculates the probability that each data point is similar to every other data points. In the second stage, t-SNE creates a map and calculates the probability that each pair of data points is close together on the map. Then, it adjusts the locations of the data points on the map to minimize the difference between the two sets of probabilities. It uses euclidean distance as a similarity measure between vectors. However, in this thesis, we will use a variant of t-SNE that uses riemannian geometry (and therefore the riemannian distance). This is called UMAP.

PCA

:   PCA creates a new set of variables out of the original dimensions in the vector. Each new variable is called a principal component. It works by creating a linear combination of the original variables, and they are chosen in such a way that the first principal component explains as much of the variation in the data as possible, the second principal component explains as much of the remaining variation as possible, and so on. Unlike t-SNE, it is linear. In order to reduce the dimension, we can choose the number of principal components to keep.

    Mathematically, the new variables are a linear combination of the original variables. Let $X$ be an $n \times p$ matrix, meaning there are $n$ samples or observations and $p$ variables or features. The first principal component, denoted as $z_1$, is the linear combination of the original variables that has the largest variance. Mathematically, $z_1$ can be defined as:

    $$
    z_1 = a_{11}\times x_1 + a_{12} \times x_2 + ... + a_{1p} \times x_p   
    $$

    where $a_{11}, ... a_{1p}$ are the coefficients of the linear combination. They are chosen in such a way that the theoretical variance of the estimator $z_1$ is maximized. Then, $z_2$ is estimated in the same way, except that the coefficients $a_{21}, a_{22}, ..., a_{2p}$ are chosen so that the theoretical variance is maximized *under the constraint* that $z_2$ is orthogonal to $z_1$.

#### *Clustering*

Clustering is a technique for grouping similar objects or data points together. It's a way to find patterns and structure in data. This can allow us to find out if sentences with similar meanings or sentiment are grouped together in the space. Here, we will give a short summary of the definition of k-means clustering, which is the most popular clustering methor. The intuition behind k-means is that it's a way to find patterns and structure in data. By grouping similar data points together, we can get a sense of what the data looks like, and what the underlying relationships between the variables are

The algorithm works by first assigning each data point to a cluster at random. Then, it calculates the centroid, or the center of mass, of each cluster, and assigns each data point to the cluster whose centroid is closest. This process is repeated until the clusters no longer change, or until a certain number of iterations have been reached.

The math behind k-means is all about minimizing the sum of the squared distances between each data point and the centroid of its cluster. The algorithm iteratively updates the centroids and the assignments of the data points to the clusters, in such a way that this objective function is minimized. We will give a short mathematical definition of the most naïve version of k-means, in order to give the reader some basic understanding of the algorithm. It proceeds in two steps that are repeated until convergence.

1.  *Assignment step :* As per its name, it consists in assigning a cluster to each observation. It decides which cluster to assign to an observation by finding the cluster with the least squared euclidian distance to the observation. This means that we are separating the space into the number of clusters, according to the observation means. Let $\bar{x}_i$ denote the $i^{\text{th}}$ mean amongst the set of initial random k means. We can write the new clusters as : $$S_i = \{ x_p : \lVert x_p - \bar{x}_i \rVert  \le \lVert x_p - \bar{x}_j \rVert, \forall j,i  \text{   s.t.} 1 \le j,i \le k    \}$$ we can see that they are defined by the set of euclidian distances that are smaller than for the cluster next to it.

2.  *Update step* : We then re-calculate the means for each cluster which gives us a new set of $\bar{x}_i, i \in \{1,..., k\}$.

Then, we can say that the algorithm has converged when the assignment to a cluster for each observation stays the same from on step to the other, for all observations.

## Analyzing embeddings

### [Geometry of sentiment]{.underline}

The goal of this part is to see if large language models understand or encode financial sentiment. For this, we will proceed in two parts, that check two different ways in which the models could encode financial sentiment.

1.  K-means clustering : the model could put different sentiments in different places in space, in which case we would see clusters that correspond to the labels of sentiment.

2.  Similarity between vectors : the model could encode a linear transform that corresponds to the space between a positive or a negative sentence. In this scenario, we should be able to evaluate the similarities and the differences between sentence embeddings with positive and negative sentiments. This can be done using the similarity metrics we mentioned earlier.

Both of these analysis are flawed in their own way. The first one forces us to compare the results an unsupervised clustering algorithm with actual labels, which might not mean that labels don't correspond to clusters of their own. The second one can only work on the condition that we match sentences together. However, it would be much more significant if we could analyze the embeddings of two sentences which are extremely similar except for their opposite sentiments. In a mild correction to this, we will sort both datasets by sentence lengths, and then select sentences that have different sentiment yet that have words in common between them.

#### *Presenting the database*

For this part of the project, we needed a large, clean and labeled database with an approximately equal amount of sentences from each sentiment. Therefore, we decided to use the Stanford Sentiment Treebank, which is comprised of approximately 67 000 movie reviews. Each review was attributed a sentiment by three human judges.

![Distribution of sentences](C:\Users\faune\Desktop\thesis\images\stanford_hist.png){fig-align="center" width="50%"}

We can also take a look at the length of the sentences, since that will be important later in our analysis.

![Sentence length by label](C:\Users\faune\Desktop\thesis\images\nb_words_hist.png){fig-align="center" width="50%"}

We can see that the distribution of sentence lengths is approximately the same for both negatively and positively labeled sentences.

#### *Similarity between vectors*

What we call here the analysis of similarity between vectors means that we are trying to find out if embeddings of similar sentences that have different sentiments either *point in the same direction* (cosine similarity), or *are of comparable length* (euclidean distance and/or norm differnce). The challenge here, and also the flaw which renders our results difficult to analyze, is that we need to find a way to match sentences with negative sentiment to corresponding sentences with positive sentiment. The way that we do this here is by finding sentences that have the highest number of matching words. This means that we are trying to maximize the length of the union of the sentences. However, this naïve approach would give an unfair advantage to longer sentences during the matching process. Therefore, we separate sentences based on the number of words in them. We then filter the sentences based on the ratio of matching words to the total number of words.

This seems simple enough, but it is harder to do in practice, given that we have around 30 000 sentences to match with one another, where we have to compare each sentence to all sentences of opposite sentiment. In order to do this efficiently, we have written a small algorithm for this task. We will quickly explain its mechanism.

1.  **Preprocessing**: we take all of our sentences and transform them into a list of "important" or significant words. Keep in mind that this removal of stop words and punctuation is not perfect, and some stop words could still be included in the final result. We also use a stemmer to find only the root of each word. For example, "parental" would become "parent". Therefore the sentence: "My grandma, an elderly lady, could do better !!!" becomes \[grandma, elder, lady, could, do, better\].

2.  **Encoding vocabulary**: We then make a corpus out of the union of all of our unique words. This represents the total vocabulary of the database. This is used in order to be able to write each sentence as a vector of ones and zeros. We will illustrate why we are doing this with a simple example. Imagine that our database contains three sentences which are "I love dogs" "I hate cats" and "it's raining cats and dogs". With our preprocessing, the corpus would be: {hate, love, cats, dogs, rain}. The first sentence thus becomes {0,1,0,1,0}, the sencond becomes {1,0,1,0,0}, and the third becomes {0,0,1,1,1}. We can see now that the dot product of two vectors compute their similarity. For example: $(0,0,1,1,1) \cdot (0,1,0,1,0)^T = 0+0+0+1+0 =1$. They therefore have one word in common. With these new vectors, we can now construct a matrix of dimensions $S \times W$ where $S$ is the number of sentences and $W$ is the number of words. Then, by computing the dot product of any two matrices that we want to compare, we are left with an $S \times S'$ matrix where each row corresponds to all of the similarity scores with all sentences in the second matrix for one sentence in the first matrix. Then, all we have to do is select the maximum value of this row, and find which sentence this corresponds to.

3.  **Computing quantiles**: However, we noticed that this method of naïvely matching sentences by words would give an unfair advantage to longer sentences, which would be matched more often. We therefore decided to split our data according to the quantiles of sentence lengths, and then compare sentences to each other within the quantiles. So, the top 10% longest negative sentences would be compared to the top 10% longest positive sentences. We can also notice that this operation of comparing similar words is not symmetric. Here, we decided to start with positive sentences.

Once the sentences are matched together we can try to compare their embeddings to see if the vectors are similar or not. The idea is that, since we matched them by words but with a different context (one sentiment being positive and the other negative), this should be reflected in the sentence embeddings. This means that there would be a consistend and similar difference between all of the matched sentence embeddings. In order to see if this is the case, we plotted the histograms of the three metrics that we want to evaluate.

**Cosine similarity**

![Cosine similarity between matched sentences embeddings](images/cosine_similarity.png){fig-align="center" width="50%"}

Interestingly, we can see that there are no values under 0.65 for the cosine similarity between our vectors. This therefore means that they are all pointing in the same direction. This is probably due to the fact that we took sentences with similar words. This can lead us to a first hypothesis that vectors will point in the same direction but have different lengths, i.e. that there will be a consistent and significant difference between their norms. This would mean that going from one sentiment to the other would be like shrinking or enlarging the vector. We therefore show the histogram of the norm differences :

![Norm difference between matched sentences](images/norm_difference.png){fig-align="center" width="50%"}

Unfortunately, we can see that the vectors are very close together, and that the mean of the difference between their euclidian norms is very close to zero. This could simply be because of all of the normalization that the model performs while generating the embeddings. Normalization tends to bring vectors closer together. Now, our last hypothesis could be that the euclidean distance between vectors is consistent. This would mean that going from one sentiment to the other is like adding the average euclidean distance.

\newpage

![Euclidean distance between matched sentences](images/euclidean_distance.png){fig-align="center" width="50%"}

Here, we can see that despite a few outliers, the distribution of the euclidean distance is relatively normal. This result could be interpreted in different ways as it is not totally convincing. However, keep in mind that our preprocessing and our initial database are not perfect. This means that we could either take these results as face value and say that they are good despite the flaws, or that the flaws are at the origin of the results.

We will show the specific statistics or our results to give the reader a little bit more insight:

| Statistics | Cosine Similarity | Euclidean Distance | Norm Difference |
|------------|-------------------|--------------------|-----------------|
| mean       | 0.7769            | 0.658064           | 6.8121e-7       |
| std        | 0.056731          | 0.114702           | 0.000259        |
| min        | 0.665127          | 0.045848           | -0.000789       |
| max        | 0.998949          | 0.818495           | 0.000765        |
| median     | 0.766584          | 0.683175           | -0.000001       |

#### *Analyzing clusters*

Here, we will try to see if the clusters that are mathematically formed by the dispersion of the embeddings in the space recreate the sentiment labels. We will also visualize the results using UMAP, a dimension reduction techniques presented earlier.

We will start by quickly presenting the mathematical methods that we will use to compare the results. Mainly, we will use the Rand Index, the Mutual Info Score, and their adjusted values. We can give the following definitions for each.

The Rand Score

:   The Rand Score is based on the idea of counting the number of pairs of data points that are either in the same cluster or in different clusters in both clusterings, and then dividing by the total number of pairs of data points.The math behind the Rand Score is quite straightforward. Let's say we have two clusters, A and B, of the same set of $n$ data points. We can define the following four quantities:

    -   a : the number of pairs of data points that are in the same cluster in A and in the same cluster in B

    -   b : the number of pairs of data points that are in different clusters in A and in different clusters in B

    -   c:the number of pairs of data points that are in the same cluster in A and in different clusters in B

    -   d : the number of pairs of data points that are in different clusters in A and in the same cluster in B

:   We can then write the formula for the Rand Index as: $$
            RS = \frac{a + b}{a + b + c+d}
            $$

    Its values are therefore between 0 and 1, with a value of 1 indicating that the clusters are identical. However, it doesn't take into account change (the fact that pairs of points could be similar by chance). Therefore, we also need to compute the adjusted rand index.The basic idea behind this is to subtract the expected value of the Rand Score from the observed value, and then to divide by the maximum possible value of the Rand Score. This ensures that the Adjusted Rand Score is always between 0 and 1, and that it is only high when the observed similarity between the clusterings is much higher than what would be expected by chance.

Mutual Information Score

:   The math behind the mutual information score is a little bit more complex, but we will start by explaining it intuitively. It is supposed to be a measure of the amount of information that can be obtained about one cluster by knowing the other clustering. It is based on the idea of comparing the joint probability distribution of the two clusters to the product of their marginal probability distributions. We calculate the entropy of each clustering, and then subtract the entropy of the joint clustering from the sum of the individual entropies. This gives us the following formula : $$
    MI(U,V)=\sum_{i=1}^{\|U\|} \sum_{j=1}^{\|V\|} \frac{\|U_i\cap V_j\|}{N} \log\frac{N\|U_i \cap V_j\|}{\|U_i\|\|V_j\|}
    $$

    Here, $U$ and $V$ denote the two clusters.

    Just like the rand score, its value goes from 0 to 1 with 1 meaning that the clusters are identical. We also have the Adjusted Mutual Information Score to account and correct for chance, which we obtain by substracting by the expected value and then dividing by the maximum possible value.

Now that we have explained out metrics, we can proceed to showing the graphs that we have found as outputs. Please note that results can vary according to clustering techniques.

![Embeddings with labels as hue](images/labels_scatterplot.png){fig-align="center" width="50%"}

![Embeddings with clusters as hue](images/clusters_scatterplot.png){fig-align="center" width="50%"}

\newpage

This therefore shows that the labels seem to more or less replicate the values that we get from unsupervised clustering techniques. Please note that we have computed clusters with k-means, the technique presented in the first part of this thesis.

We can now compute the metrics for those two clusters and analyze the results:

|                                   |                    |
|-----------------------------------|--------------------|
| Rand Index                        | 0.8613280876841903 |
| Adjusted Rand Index               | 0.7225892608139667 |
| Mutual Information Score          | 0.4201820741688528 |
| Adjusted Mutual Information Score | 0.6131630448257939 |

We can therefore see that this analysis yields good results. This is not entirely incompatible with the previous results on matched sentences. Indeed, here, we used the entirety of our database, which means more samples. We also did not have the problem of matching sentences together with an at-home algorithm. However, we can also hypothesize that matching sentences are all on the frontier that we see in between the two clusters. This means that they are really close and, although they belong to two different clusters, we can't see any fundamental difference between the vectors as is. Keep in mind however that k-means clustering on data which has no evident clusters will, in this case, always return something, because in the scikit-learn function, there is a maximum number of iterations after which the algorithm just stops and returns the results. This means that, if we imagine that the results are exactly like the two dimensional representation of them, the clusters could be placed anywhere (if for example we imagine that the frontier between them turns to denote any other separation of the "circle" of the scattered points). However, since we have clustered on high dimensional data, it is more likely that there are clusters that we can't actually see in three dimensions.

Please note however that k-means is unsupervised and highly depends on the initial random seed. Also, we clustered the high dimensional data, and we show to labels assigned during high-dimensional clustering to a 2 dimensional representation using UMAP for dimension reduction. The metrics have also been computed on high-dimensional data.

### [Link between sentence embeddings and markets]{.underline}

In this part, we will try to see if raw or reduced sentence embeddings can be used to predict the close to close daily returns of the MSCI World Index. Indeed, our database is comprised of a daily plethora of financial news headlines which can be in English or even in other languages such as Chinese. They are news about the entire world and we do not have an effective way of mapping them to a specific country. Therefore, we will generate embeddings daily from all of those sentences. The goal will be the following. First of all, to see if we can visualize a historical trend in the positions of embeddings in the space. For example, by coloring the embeddings according to years or quarters. This will help us to see if the disposition of embeddings have anything to do with financial crises or particuliar market conditions. Unfortunately, our database only starts at the beginning of 2008 and ends at the end of 2015.

Then, we will use linear regressions (first off, ordinary least squares, and then with a variety of dimension reduction techniques) to see if embeddings have an explanatory or predictive power on markets.

#### *Presenting the database*

As previously mentioned, the database starts on the 8th of August 2008 and ends on December 10th, 2015. We will very quickly give the reader an idea of the challenges that the database presented and why.

```{python}
import polars as pl 
news = (
    pl.read_csv(
        r"C:\Users\faune\Downloads\lab1\lab1\data\dow_jones_news.csv", 
        separator=';'
    )
    .with_columns(
        pl.col('news').str.split('***')
    )
)
display(
    news
    .with_columns(
        pl.col('news').list.len().alias('nb_news'),
        pl.col('news').list.join('.').str.len_chars().alias('total_length')
    )
    .drop('news','Date')
    .describe()
)
```

Here, we can see that there are a large number of sentences each day, and that the total length of the news can be very high. This poses a problem for generating embeddings, as we are limited by the context length of the model. The maximum character length of the model for one input is 8192. Therefore, we decided to limit the number of strings to 8191, and then cut to the latest end of the sentence using a special character that is very unlikely to appear in usual sentences, but is only one character long.

This means that we arbitrarily cut some headlines out of the equation, but this was the only way to solve this problem in order to get one embedding per day. This also means that there are less variations in the daily amount of news. With this new technique, we can see that we have a much more manageable database :

```{python}
news = news.sort('Date').with_columns(pl.col('news').list.join('|'))
news = (
    news.with_columns(
        pl.col('news').str.len_chars().alias('length')
    )
    .with_columns(
        pl.when(pl.col('length').ge(8192))
        .then(pl.col('news').str.head(8191))
        .otherwise(pl.col('news')).alias('stripped_news')
    )
    .with_columns(
        pl.col('stripped_news')
        .str.split('|')
        .list.head(
            pl.col('stripped_news').str.split('|').list.len()-1
        ).list.join('|')
    )
    .with_columns(pl.col('stripped_news').str.len_chars().alias('stripped_length'))
)
display(
    news
    .with_columns(
        pl.col('stripped_news').str.split('|').list.len().alias('nb_news'),
    )
    .drop('news', 'stripped_news', 'length', 'Date')
    .describe()
)
```

No additional preprocessing has been done before generating embeddings. We can see that we went from an average of approximately 800 news a day, to an average of a little over 100 headlines a day. However, we also significantly reduced the variance from 55 to 9. This is important in order to isolate the effect that the content of the news has on markets.

\newpage

#### *Visualizing embeddings*

Here, we will simply plot the sentence embeddings and try to find out which point relates to which period in time, in order to see if there is any correlation.

![Sentence embeddings position per year](images/embeddings_year.png){fig-align="center" width="50%"}

Here, we can see that there is almost an uncanny relationship between the year and the embeddings. Please note that there are mentions of the current year in the embeddings, but such mentions are very limited. Sentences also contain mentions of other years, in the form of predictions about the future. Clearly, embeddings are shifting from the right to the left as time goes on. We can also see that they are getting closer together. However, it is possible that this could be the result of the number of news per day. In order to check if there is a trend in the number of news, we plot it.

![Number of news per year](images/number_news_year.png){fig-align="center" width="50%"}

Here, we can see that the number of news per year is indeed decreasing, but not nearly as much as could be initially deduced from the graph.

We can then plot the sentiment label of the news.

\newpage

![Embeddings with labels](images/embeddings_label.png){fig-align="center" width="50%"}

Here, we can see that there is no particularly visible relationship between the label and the position of the embeddings in the space. If we confront these results with what we observed in the first part of this thesis, this is a particularly interesting result. Indeed, it seems that the model does encode differences between the years, but that this does not translate to differences in raw sentiment. Therefore, this must mean that either our sentiment labels are flawed, or, the model is "more intelligent" than the raw sentiment labels. It might understand some ineffable relationship in the headlines that allows it to recognize economic patterns.

We will now test this hypothesis by performing linear regressions on the MSCI World Index.

#### *Linear regressions against the MSCI World Index*

In order to get a thorough and precise analysis of the way that sentence might or might not be relevant to analyze market moves, we will conduct a number of different linear regressions. The first one will be a very naïve regression with very minimal dimension reduction on embeddings. Since the original dimension for our embeddings is 1064, we anticipate that the results of our model will be very overfit. In order to possibly correct this, we will try a few different methods:

-   Reducing the dimension by half with UMAP

-   Reducing the dimension with PCA

-   OLS with norm penalization :

    -   Ridge

    -   Lasso

    -   Elastic Net

We will also try a mix of those models. Lastly, we will try this against the close to close returns as well as the volatility of the index.

In this part, we will only present the models that yield the best results, as there are a number of different models to try and most of them will probably not be adding anything significant to our analysis. Let us first show the results of the most naïve version of the regression, so that we know what we are working with.

![Embeddings against MSCI World daily returns](images/first_reg.png)

Here, we can see that the model is obviously wildly overfit. We will therefore try to do the same analysis with a Ridge penalization, as previously mentioned.

![Ridge regression, Embeddings against MSCI World daily returns](images/ridge_reg.png)

Here, we can see that there is effectively no effect and no relationship between the data.

### [Comparing results]{.underline}

## Conclusion

## Bibliography

## Annexe